# -*- coding: utf-8 -*-
"""MajorCodeFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TYXv_CLs3CrLlGzmo-ckCX1CjIeOopqc
"""

!pip install textract

import textract
import os
from os import listdir
from os.path import isfile, join
from io import StringIO
import pandas as pd
from collections import Counter
import en_core_web_sm
nlp = en_core_web_sm.load()
from spacy.matcher import PhraseMatcher

mydirectory='Candidate Resume' #enter your path here where you saved the resumes
onlyfiles = [os.path.join(mydirectory, f) for f in os.listdir(mydirectory) if os.path.isfile(os.path.join(mydirectory, f))]

def pdfextract(file):
    text = textract.process(file, method='pdfminer')

    return text

#function that does phrase matching and builds a candidate profile
def create_profile(file):
    text = pdfextract(file) 
    text = str(text)
  
    #below is the csv where we have all the keywords
    keyword_dict = pd.read_csv('template.csv')
    print('The templates on which assessment will be done \n\n\n',keyword_dict.head())
    
    #Creating Directory for individual words
    openness_words = [nlp(text) for text in keyword_dict['openness'].dropna(axis = 0)]             
    neuroticism_words = [nlp(text) for text in keyword_dict['neuroticism'].dropna(axis = 0)]
    conscientiousness_words = [nlp(text) for text in keyword_dict['conscientiousness'].dropna(axis = 0)]
    agreeableness_words = [nlp(text) for text in keyword_dict['agreeableness'].dropna(axis = 0)]
    extraversion_words = [nlp(text) for text in keyword_dict['extraversion']]
       
    matcher = PhraseMatcher(nlp.vocab)
    matcher.add('Openness', None, *openness_words)
    matcher.add('Neuroticism', None, *neuroticism_words)
    matcher.add('Conscientiousness', None, *conscientiousness_words)
    matcher.add('Agreeableness', None, *agreeableness_words)
    matcher.add('Extraversion', None, *extraversion_words)
    doc = nlp(text)
    
    d = []  
    print(d)
    matches = matcher(doc)
    
    for match_id, start, end in matches:
        rule_id = nlp.vocab.strings[match_id]  # get the unicode ID, i.e. 'COLOR'
        span = doc[start : end]  # get the matched slice of the doc
        d.append((rule_id, span.text))
      
    keywords = "\n".join(f'{i[0]} {i[1]} ({j})' for i,j in Counter(d).items())

    
    ## convertimg string of keywords to dataframe
    df = pd.read_csv(StringIO(keywords),names = ['Keywords_List'])
    df1 = pd.DataFrame(df.Keywords_List.str.split(' ',1).tolist(),columns = ['Subject','Keyword'])
    df2 = pd.DataFrame(df1.Keyword.str.split('(',1).tolist(),columns = ['Keyword', 'Count'])
    df3 = pd.concat([df1['Subject'],df2['Keyword'], df2['Count']], axis =1) 
    df3['Count'] = df3['Count'].apply(lambda x: x.rstrip(")"))
    
    base = os.path.basename(file)
    filename = os.path.splitext(base)[0]
       
    name = filename.split('_')
    name2 = name[0]
    name2 = name2.lower()
    ## converting str to dataframe
    name3 = pd.read_csv(StringIO(name2),names = ['Candidate Name'])
    
    dataf = pd.concat([name3['Candidate Name'], df3['Subject'], df3['Keyword'], df3['Count']], axis = 1)
    dataf['Candidate Name'].fillna(dataf['Candidate Name'].iloc[0], inplace = True)

    return(dataf)

final_database = pd.DataFrame()

i = 0 
while i < len(onlyfiles):
    file = onlyfiles[i]
    dat = create_profile(file)
    final_database = final_database.append(dat)
    i +=1
    
    
#code to count words under each category and visulaize it through Matplotlib
final_database2 = final_database['Keyword'].groupby([final_database['Candidate Name'], final_database['Subject']]).count().unstack()
final_database2.reset_index(inplace = True)
final_database2.fillna(0,inplace=True)
print(final_database2)
new_data = final_database2.iloc[:,1:]
new_data.index = final_database2['Candidate Name']

final_database2

list1=['Openness', 'Neuroticism ', 'Conscientiousness','Agreeableness','Extraversion']

for i in list1:
  if i not in final_database2.columns[1:]:
    final_database2[i]=0

final_database2

import matplotlib.pyplot as plt
plt.rcParams.update({'font.size': 10})
ax = new_data.plot.barh(title="Resume keywords by category", legend=False, figsize=(15,7), stacked=True)

labels = []
for j in new_data.columns:
    for i in new_data.index:
        label = str(j)+": " + str(new_data.loc[i][j])
        labels.append(label)
patches = ax.patches
for label, rect in zip(labels, patches):
    width = rect.get_width()
    if width > 0:
        x = rect.get_x()
        y = rect.get_y()
        height = rect.get_height()
        ax.text(x + width/2., y + height/2., label, ha='center', va='center')
plt.show()

dic1={}
for i in labels:
   temp1,temp2=i.split(' ')
   temp2=int(temp2)
   dic1[temp1]=temp2

df3=pd.DataFrame(dic1.items(),columns=['Trait','Trait Score'])

import seaborn as sns
plt.figure(figsize=(12,6))
sns.barplot(x='Trait',y='Trait Score',data=df3)
plt.ylim(0,5)

import pandas as pd
from numpy import *
import numpy as np
from sklearn import preprocessing
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, confusion_matrix, classification_report
from sklearn import metrics
from sklearn import neighbors

data =pd.read_csv('train dataset.csv').drop(columns = ['Gender', 'Age', ])

x_train = data.drop(columns = ['Personality (Class label)'])
y_train = data['Personality (Class label)']

test =pd.read_csv('test dataset.csv').drop(columns = ['Gender', 'Age', ])

x_test = test.drop(columns = ['Personality (class label)'])
y_test = test['Personality (class label)']
              
data.head()

model = linear_model.LogisticRegression(multi_class='multinomial', solver='newton-cg',max_iter =1000)
model.fit(x_train, y_train)

y_pred = model.predict(x_test)
model.score(x_test, y_test) 

print('Accuracy score: ', accuracy_score(y_test, y_pred))

print(classification_report(y_test, y_pred))

print(confusion_matrix(y_test, y_pred))

##SVM
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
scaler=StandardScaler()
x_standard_train=scaler.fit_transform(x_train)
x_standard_test=scaler.fit_transform(x_test)
x_train_f=pd.DataFrame(x_standard_train)
x_test_f=pd.DataFrame(x_standard_test)
model = SVC(kernel='linear',gamma=10.0,C=1, probability=True)
model.fit(x_train_f, y_train)

y_train.value_counts()

y_pred = model.predict(x_test_f)
model.score(x_test_f, y_test)

print('Accuracy score: ', accuracy_score(y_test, y_pred))

print(classification_report(y_test, y_pred))

print(confusion_matrix(y_test, y_pred))

final_prediction=model.predict(final_database2.drop(labels=['Candidate Name'], axis=1))

final_prediction

model.classes_

plotted= pd.DataFrame(data=final_prediction.T, index=model.classes_.T)

plotted.plot(kind='pie', subplots=True, legend=None, autopct='%1.1f%%')
plt.axis('equal')
plt.savefig('pie.jpg')



print('The Nature of the given Candidate based on our model is',''.join(final_prediction))

from joblib import dump

dump(model, filename="model_svm.joblib")

